{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio transformers datasets jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, Wav2Vec2Processor, TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_metric, Audio, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "BASE_PATH = \"YOUR_BASE_PATH\" \n",
    "\n",
    "AUDIO_DIR = \"BASE_PATH\\common_voice\\cv-valid-train\"\n",
    "CSV_FILE = \"BASE_PATH\\common_voice\\cv-valid-train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justication for Tokenizer: \n",
    "- wav2vec2-base-960h is pre-trained on a massive dataset of 960 hours of speech data from LibriSpeech. This extensive training provides the model with a strong foundation in understanding the nuances of spoken language, including phoneme and word-level representations.\n",
    "- Starting with a pre-trained model allows to leverage the power of transfer learning. Fine-tuning the pre-trained model on the specific dataset requires less training data and computational resources compared to training a model from scratch.\n",
    "\n",
    "Justication for Feature Extractor: \n",
    "- The Wav2Vec2 processor wraps a feature extractor and a tokenizer into a single processor, offering all the functionalities both. It takes care of both feature extraction from audio and text label preparation, streamlining the data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:752: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Wav2Vec2 model and tokenizer\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Create processor pipeline (Feature extractor built into processor)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cv-valid-train contains almost 200K files (195,776)\n",
    "- With respect to compute power contraints, random subset of cv-valid-train data is then used, which is then further split into 70-30 train-validation\n",
    "- Subset was then randomly selected as 20K records (20K/200K = 10%) among dataset, which is justifiably a decent subsample of the dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-train/sample-000000.mp3</td>\n",
       "      <td>learn to recognize omens and follow them the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-train/sample-000001.mp3</td>\n",
       "      <td>everything in the universe evolved he said</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-train/sample-000002.mp3</td>\n",
       "      <td>you came so that you could learn about your dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-train/sample-000003.mp3</td>\n",
       "      <td>so now i fear nothing because it was those ome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-train/sample-000004.mp3</td>\n",
       "      <td>if you start your emails with greetings let me...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195771</th>\n",
       "      <td>cv-valid-train/sample-195771.mp3</td>\n",
       "      <td>the englishman said nothing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>male</td>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195772</th>\n",
       "      <td>cv-valid-train/sample-195772.mp3</td>\n",
       "      <td>the irish man sipped his tea</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195773</th>\n",
       "      <td>cv-valid-train/sample-195773.mp3</td>\n",
       "      <td>what do you know about that</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195774</th>\n",
       "      <td>cv-valid-train/sample-195774.mp3</td>\n",
       "      <td>the phone rang while she was awake</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195775</th>\n",
       "      <td>cv-valid-train/sample-195775.mp3</td>\n",
       "      <td>among these people were a couple of cyclists a...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195776 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "0       cv-valid-train/sample-000000.mp3   \n",
       "1       cv-valid-train/sample-000001.mp3   \n",
       "2       cv-valid-train/sample-000002.mp3   \n",
       "3       cv-valid-train/sample-000003.mp3   \n",
       "4       cv-valid-train/sample-000004.mp3   \n",
       "...                                  ...   \n",
       "195771  cv-valid-train/sample-195771.mp3   \n",
       "195772  cv-valid-train/sample-195772.mp3   \n",
       "195773  cv-valid-train/sample-195773.mp3   \n",
       "195774  cv-valid-train/sample-195774.mp3   \n",
       "195775  cv-valid-train/sample-195775.mp3   \n",
       "\n",
       "                                                     text  up_votes  \\\n",
       "0       learn to recognize omens and follow them the o...         1   \n",
       "1              everything in the universe evolved he said         1   \n",
       "2       you came so that you could learn about your dr...         1   \n",
       "3       so now i fear nothing because it was those ome...         1   \n",
       "4       if you start your emails with greetings let me...         3   \n",
       "...                                                   ...       ...   \n",
       "195771                        the englishman said nothing         1   \n",
       "195772                       the irish man sipped his tea         1   \n",
       "195773                        what do you know about that         1   \n",
       "195774                 the phone rang while she was awake         2   \n",
       "195775  among these people were a couple of cyclists a...         4   \n",
       "\n",
       "        down_votes       age gender   accent  duration  \n",
       "0                0       NaN    NaN      NaN       NaN  \n",
       "1                0       NaN    NaN      NaN       NaN  \n",
       "2                0       NaN    NaN      NaN       NaN  \n",
       "3                0       NaN    NaN      NaN       NaN  \n",
       "4                2       NaN    NaN      NaN       NaN  \n",
       "...            ...       ...    ...      ...       ...  \n",
       "195771           0  thirties   male  england       NaN  \n",
       "195772           0       NaN    NaN      NaN       NaN  \n",
       "195773           0       NaN    NaN      NaN       NaN  \n",
       "195774           0  twenties   male       us       NaN  \n",
       "195775           1       NaN    NaN      NaN       NaN  \n",
       "\n",
       "[195776 rows x 8 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from CSV\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182715</th>\n",
       "      <td>cv-valid-train/sample-182715.mp3</td>\n",
       "      <td>the fine manufacturing company was a bookkeepe...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114956</th>\n",
       "      <td>cv-valid-train/sample-114956.mp3</td>\n",
       "      <td>the mixture took on a reddish color almost the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fourties</td>\n",
       "      <td>male</td>\n",
       "      <td>england</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139880</th>\n",
       "      <td>cv-valid-train/sample-139880.mp3</td>\n",
       "      <td>enough said the boy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34538</th>\n",
       "      <td>cv-valid-train/sample-034538.mp3</td>\n",
       "      <td>then he sat in the sunfilled doorway smoking t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142907</th>\n",
       "      <td>cv-valid-train/sample-142907.mp3</td>\n",
       "      <td>i've just guaranteed the bank sufficient funds...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115399</th>\n",
       "      <td>cv-valid-train/sample-115399.mp3</td>\n",
       "      <td>what's she saying now</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118095</th>\n",
       "      <td>cv-valid-train/sample-118095.mp3</td>\n",
       "      <td>would you like a cappuccino</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84886</th>\n",
       "      <td>cv-valid-train/sample-084886.mp3</td>\n",
       "      <td>how come you speak spanish he asked</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135792</th>\n",
       "      <td>cv-valid-train/sample-135792.mp3</td>\n",
       "      <td>it's a man who understands nature and the world</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39598</th>\n",
       "      <td>cv-valid-train/sample-039598.mp3</td>\n",
       "      <td>but none of that is from the pyramids said the...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "182715  cv-valid-train/sample-182715.mp3   \n",
       "114956  cv-valid-train/sample-114956.mp3   \n",
       "139880  cv-valid-train/sample-139880.mp3   \n",
       "34538   cv-valid-train/sample-034538.mp3   \n",
       "142907  cv-valid-train/sample-142907.mp3   \n",
       "...                                  ...   \n",
       "115399  cv-valid-train/sample-115399.mp3   \n",
       "118095  cv-valid-train/sample-118095.mp3   \n",
       "84886   cv-valid-train/sample-084886.mp3   \n",
       "135792  cv-valid-train/sample-135792.mp3   \n",
       "39598   cv-valid-train/sample-039598.mp3   \n",
       "\n",
       "                                                     text  up_votes  \\\n",
       "182715  the fine manufacturing company was a bookkeepe...         2   \n",
       "114956  the mixture took on a reddish color almost the...         1   \n",
       "139880                                enough said the boy         1   \n",
       "34538   then he sat in the sunfilled doorway smoking t...         2   \n",
       "142907  i've just guaranteed the bank sufficient funds...         1   \n",
       "...                                                   ...       ...   \n",
       "115399                              what's she saying now         1   \n",
       "118095                        would you like a cappuccino         3   \n",
       "84886                 how come you speak spanish he asked         1   \n",
       "135792    it's a man who understands nature and the world         1   \n",
       "39598   but none of that is from the pyramids said the...         2   \n",
       "\n",
       "        down_votes       age  gender   accent  duration  \n",
       "182715           0       NaN     NaN      NaN       NaN  \n",
       "114956           0  fourties    male  england       NaN  \n",
       "139880           0       NaN     NaN      NaN       NaN  \n",
       "34538            1       NaN     NaN      NaN       NaN  \n",
       "142907           0       NaN     NaN      NaN       NaN  \n",
       "...            ...       ...     ...      ...       ...  \n",
       "115399           0       NaN     NaN      NaN       NaN  \n",
       "118095           0       NaN     NaN      NaN       NaN  \n",
       "84886            0       NaN     NaN      NaN       NaN  \n",
       "135792           0       NaN     NaN      NaN       NaN  \n",
       "39598            0  thirties  female       us       NaN  \n",
       "\n",
       "[20000 rows x 8 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_df = df.sample(20000, replace=False, random_state=42)\n",
    "subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182715    cv-valid-train/sample-182715.mp3\n",
       "114956    cv-valid-train/sample-114956.mp3\n",
       "139880    cv-valid-train/sample-139880.mp3\n",
       "34538     cv-valid-train/sample-034538.mp3\n",
       "142907    cv-valid-train/sample-142907.mp3\n",
       "                        ...               \n",
       "115399    cv-valid-train/sample-115399.mp3\n",
       "118095    cv-valid-train/sample-118095.mp3\n",
       "84886     cv-valid-train/sample-084886.mp3\n",
       "135792    cv-valid-train/sample-135792.mp3\n",
       "39598     cv-valid-train/sample-039598.mp3\n",
       "Name: filename, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listnames = subset_df[\"filename\"]\n",
    "listnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182715</th>\n",
       "      <td>cv-valid-train/sample-182715.mp3</td>\n",
       "      <td>the fine manufacturing company was a bookkeepe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114956</th>\n",
       "      <td>cv-valid-train/sample-114956.mp3</td>\n",
       "      <td>the mixture took on a reddish color almost the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139880</th>\n",
       "      <td>cv-valid-train/sample-139880.mp3</td>\n",
       "      <td>enough said the boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34538</th>\n",
       "      <td>cv-valid-train/sample-034538.mp3</td>\n",
       "      <td>then he sat in the sunfilled doorway smoking t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142907</th>\n",
       "      <td>cv-valid-train/sample-142907.mp3</td>\n",
       "      <td>i've just guaranteed the bank sufficient funds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115399</th>\n",
       "      <td>cv-valid-train/sample-115399.mp3</td>\n",
       "      <td>what's she saying now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118095</th>\n",
       "      <td>cv-valid-train/sample-118095.mp3</td>\n",
       "      <td>would you like a cappuccino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84886</th>\n",
       "      <td>cv-valid-train/sample-084886.mp3</td>\n",
       "      <td>how come you speak spanish he asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135792</th>\n",
       "      <td>cv-valid-train/sample-135792.mp3</td>\n",
       "      <td>it's a man who understands nature and the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39598</th>\n",
       "      <td>cv-valid-train/sample-039598.mp3</td>\n",
       "      <td>but none of that is from the pyramids said the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "182715  cv-valid-train/sample-182715.mp3   \n",
       "114956  cv-valid-train/sample-114956.mp3   \n",
       "139880  cv-valid-train/sample-139880.mp3   \n",
       "34538   cv-valid-train/sample-034538.mp3   \n",
       "142907  cv-valid-train/sample-142907.mp3   \n",
       "...                                  ...   \n",
       "115399  cv-valid-train/sample-115399.mp3   \n",
       "118095  cv-valid-train/sample-118095.mp3   \n",
       "84886   cv-valid-train/sample-084886.mp3   \n",
       "135792  cv-valid-train/sample-135792.mp3   \n",
       "39598   cv-valid-train/sample-039598.mp3   \n",
       "\n",
       "                                                     text  \n",
       "182715  the fine manufacturing company was a bookkeepe...  \n",
       "114956  the mixture took on a reddish color almost the...  \n",
       "139880                                enough said the boy  \n",
       "34538   then he sat in the sunfilled doorway smoking t...  \n",
       "142907  i've just guaranteed the bank sufficient funds...  \n",
       "...                                                   ...  \n",
       "115399                              what's she saying now  \n",
       "118095                        would you like a cappuccino  \n",
       "84886                 how come you speak spanish he asked  \n",
       "135792    it's a man who understands nature and the world  \n",
       "39598   but none of that is from the pyramids said the...  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dataframe\n",
    "df = pd.DataFrame() \n",
    "df[\"filename\"] = subset_df[\"filename\"]\n",
    "df[\"text\"] = subset_df[\"text\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification for Preprocessing:\n",
    "- Resampling to Consistent Rate: The Wav2Vec2ForCTC model expects input audio with a specific sampling rate (e.g., 16000 Hz).Inconsistent sampling rates can degrade model performance. Resampling ensures uniform input to the model, leading to more stable and accurate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def preprocess_audio(audio_dir, filename):    \n",
    "    print(audio_dir)\n",
    "    print(filename)\n",
    "    full_path = os.path.join(audio_dir, filename) \n",
    "    print(f\"Loading audio from: {full_path}\")  # Print the full path\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(full_path, sr=16000) \n",
    "        print(audio)\n",
    "        print(sample_rate)\n",
    "        \n",
    "        # Checking if resampling if necessary\n",
    "        if sample_rate != 16000:\n",
    "            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000) \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Failed to transcribe audio\" + str(e))\n",
    "\n",
    "    return audio\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).upper()\n",
    "    return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio'] = df['filename'].map(lambda x: preprocess_audio(AUDIO_DIR, x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df) \n",
    "dataset = dataset.map(remove_special_characters)\n",
    "dataset = dataset.map(prepare_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying train-test split of 70-30 to split train-val sets\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing dataset to disk then loading from there as a progress checkpoint, since earlier code took a long time to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe163a4dd98448be983255a65016ae50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/17 shards):   0%|          | 0/14000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f992f09d4404a9d7fa0f0bc6a9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"dataset_new\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['filename', 'text', 'audio', '__index_level_0__', 'input_values', 'labels'],\n",
       "        num_rows: 14000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['filename', 'text', 'audio', '__index_level_0__', 'input_values', 'labels'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"dataset_new\") \n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom data collator is beneficial in some situations, like towards Advanced Padding Strategies if more control over padding is needed. In this situation, the replacement padding is done with -100 to ignore loss correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replacing padding with -100 so as to correctly ignore loss \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: wav2vec2.masked_spec_embed | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.conv.weight | Shape: torch.Size([512, 1, 10])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight | Shape: torch.Size([512])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias | Shape: torch.Size([512])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.1.conv.weight | Shape: torch.Size([512, 512, 3])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.2.conv.weight | Shape: torch.Size([512, 512, 3])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.3.conv.weight | Shape: torch.Size([512, 512, 3])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.4.conv.weight | Shape: torch.Size([512, 512, 3])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.5.conv.weight | Shape: torch.Size([512, 512, 2])\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.6.conv.weight | Shape: torch.Size([512, 512, 2])\n",
      "Layer: wav2vec2.feature_projection.layer_norm.weight | Shape: torch.Size([512])\n",
      "Layer: wav2vec2.feature_projection.layer_norm.bias | Shape: torch.Size([512])\n",
      "Layer: wav2vec2.feature_projection.projection.weight | Shape: torch.Size([768, 512])\n",
      "Layer: wav2vec2.feature_projection.projection.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.weight_g | Shape: torch.Size([1, 1, 128])\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.weight_v | Shape: torch.Size([768, 48, 128])\n",
      "Layer: wav2vec2.encoder.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.0.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.0.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.1.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.1.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.2.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.2.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.3.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.3.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.4.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.4.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.5.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.5.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.6.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.6.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.7.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.7.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.8.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.8.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.9.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.9.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.10.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.10.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.k_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.k_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.v_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.v_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.q_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.q_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.out_proj.weight | Shape: torch.Size([768, 768])\n",
      "Layer: wav2vec2.encoder.layers.11.attention.out_proj.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight | Shape: torch.Size([3072, 768])\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias | Shape: torch.Size([3072])\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.output_dense.weight | Shape: torch.Size([768, 3072])\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.output_dense.bias | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.final_layer_norm.weight | Shape: torch.Size([768])\n",
      "Layer: wav2vec2.encoder.layers.11.final_layer_norm.bias | Shape: torch.Size([768])\n",
      "Layer: lm_head.weight | Shape: torch.Size([32, 768])\n",
      "Layer: lm_head.bias | Shape: torch.Size([32])\n",
      "Total number of parameters: 94396320\n",
      "Model size (approximately): 360.09 MB\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape}\") \n",
    "    total_params += torch.prod(torch.tensor(param.shape)).item() \n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Model size (approximately): {total_params * 4 / (1024 * 1024):.2f} MB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=768, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Development Rationale:\n",
    "1. Freezing feature extractor: Unfreezing it to fine-tune the entire model for potentially better performance effectively is \"fine-tuning from scratch\". However, freezing the feature extractor is a common and effective strategy for fine-tuning pre-trained models like Wav2Vec2. It allows for faster training, improved stability, and better generalization, especially when dealing with limited training data. Notably, freezing the feature extractor effectively leverages transfer learning. The model benefits from the pre-trained knowledge of the feature extractor, allowing it to learn the specific task (e.g., speech recognition) more quickly and effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1635: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 94396320\n",
      "Number of frozen parameters: 4200448 (4.45%)\n",
      "Number of unfrozen parameters: 90195872 (95.55%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "unfrozen_params = 0\n",
    "frozen_params = 0\n",
    "\n",
    "# Check which parameters are modified and are unfreezed\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        unfrozen_params += torch.prod(torch.tensor(param.shape)).item() \n",
    "    else:\n",
    "        frozen_params += torch.prod(torch.tensor(param.shape)).item()\n",
    "\n",
    "total_params = frozen_params + unfrozen_params\n",
    "frozen_params_percentage = (frozen_params / total_params) * 100\n",
    "unfrozen_params_percentage = (unfrozen_params / total_params) * 100\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Number of frozen parameters: {frozen_params} ({frozen_params_percentage:.2f}%)\")\n",
    "print(f\"Number of unfrozen parameters: {unfrozen_params} ({unfrozen_params_percentage:.2f}%)\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER is a widely accepted and standardized metric in the speech recognition field. This allows for consistent comparison of different ASR systems and research findings across the community. \n",
    "WER quantifies the number of errors made by the ASR system in transcribing speech. It also directly reflects how well the system aligns with human speech.\n",
    "As such, we use WER as the desired metric for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jared\\AppData\\Local\\Temp\\ipykernel_10332\\24688115.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  pred_logits = pred.predictions\n",
    "  pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "  # Padding replacement with -100 so as to correctly compute metrics\n",
    "  pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "  # Decode predictions and references\n",
    "  pred_str = processor.batch_decode(pred_ids)\n",
    "  label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "  # Calculate WER as metric \n",
    "  wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "  return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters Rationale:\n",
    "- Batch size: Since batch sizes are commonly exponents of 2, knowing that the dataset for training has been sub-sampled to 2K with 70-30 split, a batch size of 32 is then selected as it allows for sufficient training across the provided training data.\n",
    "- Learning rate: 1e-4 is a common starting learning rate for fine-tuning pre-trained language models. However, knowing that the model has already been pre-trained on a large dataset and the aim is to fine-tune towards a custom dataset, the learning rate is thereby reduced slightly to 1e-5.\n",
    "- Max steps: It allows control over the training time and computational resources more effectively; by defining a fixed number of training steps, it is more convenient and reliable than relying solely on the number of epochs. max_steps of 100 is then selected as a balance of sufficient model training plus time efficiency given limited computational resources.\n",
    "\n",
    "Note: GPU functions (eg, fp16=True) have been omitted for this assessment as local device used does not support CUDA :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=32,  \n",
    "    max_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100, \n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarification: By right this is the procedure to fine-tune an ASR model ideally with GPUs, but the above trainer is taking 6-7H per epoch on local device.\n",
    "\n",
    "Due to computing limitations, the following revisions are made:\n",
    "- Batch sizes reduced to 8 (Original Batch size of 32 leads to one epoch being ~1H, for numerous epochs this would take a while)\n",
    "- Randomly sub-sample K from validation set of 600 records; arguably this is bad practice due to risks of data leakage and overfitting, however in interests of resource constraints, K records are randomly selected to form the validation set as to observe metrics during model training; K=32 was eventually selected   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args_v2 = TrainingArguments(\n",
    "    output_dir=\"./results\", \n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,  # reduced from 32 \n",
    "    per_device_eval_batch_size=8,  # reduced from 32\n",
    "    evaluation_strategy=\"steps\",\n",
    "    max_steps=100,\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=20,\n",
    "    learning_rate=1e-5,\n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['filename', 'text', 'audio', '__index_level_0__', 'input_values', 'labels'],\n",
       "    num_rows: 32\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of samples for each evaluation subset\n",
    "num_eval_samples = 32 \n",
    "eval_results = []\n",
    "\n",
    "# Randomly select indices for the evaluation subset\n",
    "for i in range(num_eval_samples):\n",
    "    random_indices = random.sample(range(len(dataset[\"test\"])), num_eval_samples) \n",
    "    val_subset = dataset[\"test\"].select(random_indices) \n",
    "\n",
    "val_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer_v2 = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args_v2,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=val_subset,    # replced from dataset[\"test\"]\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_v2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263.0024</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>183.492935</td>\n",
       "      <td>0.150307</td>\n",
       "      <td>30.8891</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.1323</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.02</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>40</td>\n",
       "      <td>177.363770</td>\n",
       "      <td>0.144172</td>\n",
       "      <td>31.9061</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195.1968</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.03</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>60</td>\n",
       "      <td>167.607376</td>\n",
       "      <td>0.138037</td>\n",
       "      <td>31.3244</td>\n",
       "      <td>1.022</td>\n",
       "      <td>0.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>224.9356</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>165.750168</td>\n",
       "      <td>0.134969</td>\n",
       "      <td>30.9215</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216.0207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "      <td>162.698532</td>\n",
       "      <td>0.134969</td>\n",
       "      <td>30.6834</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1695.5832</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.059</td>\n",
       "      <td>5.504389e+16</td>\n",
       "      <td>221.657549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  learning_rate  epoch  step   eval_loss  eval_wer  eval_runtime  \\\n",
       "0   263.0024       0.000008   0.01    20         NaN       NaN           NaN   \n",
       "1        NaN            NaN   0.01    20  183.492935  0.150307       30.8891   \n",
       "2   209.1323       0.000006   0.02    40         NaN       NaN           NaN   \n",
       "3        NaN            NaN   0.02    40  177.363770  0.144172       31.9061   \n",
       "4   195.1968       0.000004   0.03    60         NaN       NaN           NaN   \n",
       "5        NaN            NaN   0.03    60  167.607376  0.138037       31.3244   \n",
       "6   224.9356       0.000002   0.05    80         NaN       NaN           NaN   \n",
       "7        NaN            NaN   0.05    80  165.750168  0.134969       30.9215   \n",
       "8   216.0207       0.000000   0.06   100         NaN       NaN           NaN   \n",
       "9        NaN            NaN   0.06   100  162.698532  0.134969       30.6834   \n",
       "10       NaN            NaN   0.06   100         NaN       NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                     1.036                  0.129            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                     1.003                  0.125            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                     1.022                  0.128            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                     1.035                  0.129            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                     1.043                  0.130            NaN   \n",
       "10                      NaN                    NaN      1695.5832   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                     0.472                   0.059  5.504389e+16  221.657549  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(trainer_v2.state.log_history)\n",
    "\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263.0024</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209.1323</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.02</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195.1968</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.03</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>224.9356</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>216.0207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  learning_rate  epoch  step  eval_loss  eval_wer  eval_runtime  \\\n",
       "0  263.0024       0.000008   0.01    20        NaN       NaN           NaN   \n",
       "2  209.1323       0.000006   0.02    40        NaN       NaN           NaN   \n",
       "4  195.1968       0.000004   0.03    60        NaN       NaN           NaN   \n",
       "6  224.9356       0.000002   0.05    80        NaN       NaN           NaN   \n",
       "8  216.0207       0.000000   0.06   100        NaN       NaN           NaN   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                      NaN                    NaN            NaN   \n",
       "2                      NaN                    NaN            NaN   \n",
       "4                      NaN                    NaN            NaN   \n",
       "6                      NaN                    NaN            NaN   \n",
       "8                      NaN                    NaN            NaN   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
       "0                       NaN                     NaN         NaN         NaN  \n",
       "2                       NaN                     NaN         NaN         NaN  \n",
       "4                       NaN                     NaN         NaN         NaN  \n",
       "6                       NaN                     NaN         NaN         NaN  \n",
       "8                       NaN                     NaN         NaN         NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_df = loss_df.dropna(axis=0, subset=\"loss\")\n",
    "train_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_wer</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>20</td>\n",
       "      <td>183.492935</td>\n",
       "      <td>0.150307</td>\n",
       "      <td>30.8891</td>\n",
       "      <td>1.036</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>40</td>\n",
       "      <td>177.363770</td>\n",
       "      <td>0.144172</td>\n",
       "      <td>31.9061</td>\n",
       "      <td>1.003</td>\n",
       "      <td>0.125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>60</td>\n",
       "      <td>167.607376</td>\n",
       "      <td>0.138037</td>\n",
       "      <td>31.3244</td>\n",
       "      <td>1.022</td>\n",
       "      <td>0.128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05</td>\n",
       "      <td>80</td>\n",
       "      <td>165.750168</td>\n",
       "      <td>0.134969</td>\n",
       "      <td>30.9215</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06</td>\n",
       "      <td>100</td>\n",
       "      <td>162.698532</td>\n",
       "      <td>0.134969</td>\n",
       "      <td>30.6834</td>\n",
       "      <td>1.043</td>\n",
       "      <td>0.130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loss  learning_rate  epoch  step   eval_loss  eval_wer  eval_runtime  \\\n",
       "1   NaN            NaN   0.01    20  183.492935  0.150307       30.8891   \n",
       "3   NaN            NaN   0.02    40  177.363770  0.144172       31.9061   \n",
       "5   NaN            NaN   0.03    60  167.607376  0.138037       31.3244   \n",
       "7   NaN            NaN   0.05    80  165.750168  0.134969       30.9215   \n",
       "9   NaN            NaN   0.06   100  162.698532  0.134969       30.6834   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "1                    1.036                  0.129            NaN   \n",
       "3                    1.003                  0.125            NaN   \n",
       "5                    1.022                  0.128            NaN   \n",
       "7                    1.035                  0.129            NaN   \n",
       "9                    1.043                  0.130            NaN   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second  total_flos  train_loss  \n",
       "1                       NaN                     NaN         NaN         NaN  \n",
       "3                       NaN                     NaN         NaN         NaN  \n",
       "5                       NaN                     NaN         NaN         NaN  \n",
       "7                       NaN                     NaN         NaN         NaN  \n",
       "9                       NaN                     NaN         NaN         NaN  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss_df = loss_df.dropna(axis=0, subset=\"eval_loss\")\n",
    "eval_loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10,6))\n",
    "\n",
    "axs[0].plot(train_loss_df[\"step\"], train_loss_df[\"loss\"])\n",
    "axs[0].set_title(\"Training Loss\")\n",
    "axs[0].set_xlabel('Steps')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(eval_loss_df[\"step\"], eval_loss_df[\"eval_loss\"])\n",
    "axs[1].set_title(\"Validation Loss\")\n",
    "axs[1].set_xlabel('Steps')\n",
    "axs[1].set_ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of Results:\n",
    "- Validation loss OK, with loss decreasing over time\n",
    "- Training loss generally decreasing, but could be better with slight increase then decrease over the halfway mark\n",
    "- Do note training procedure was modified compared to original intentions due to computational resource limitations\n",
    "- Ideally in expected model training, training and validation loss should converge over a long time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "FT_ASR_MODEL = 'wav2vec2-large-960h-cv'\n",
    "trainer_v2.save_model(FT_ASR_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
